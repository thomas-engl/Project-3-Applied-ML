{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc79994",
   "metadata": {},
   "source": [
    "# Test of Network architecture in the 1D case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b5f71",
   "metadata": {},
   "source": [
    "We test different numbers of hidden layers and nodes per layers and analyze which architecture leads to the best results. Here, only the one-dimensional heat equation is considered. We expect that the findings can be more or less used in the two-dimensional case as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c573426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pinn import heat_nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca5ae6",
   "metadata": {},
   "source": [
    "We consider the one-dimensional heat equation\n",
    "$$\n",
    "\\partial_t u - \\kappa \\Delta u = f\n",
    "$$\n",
    "where $\\kappa = 0.1$ and $f(x) = \\sin (\\pi x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cb0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PDE parameters\n",
    "\n",
    "# dimension\n",
    "dim=1\n",
    "\n",
    "# diffusion coefficient\n",
    "kappa = 0.1\n",
    "\n",
    "# initial condition\n",
    "u_0 = lambda x: torch.sin(torch.pi * x) + torch.sin(4 * torch.pi * x)\n",
    "\n",
    "# right hand side of the equation\n",
    "rhs = lambda x, t: torch.sin(torch.pi * x)\n",
    "\n",
    "# analytic solution\n",
    "u_analytic = lambda x, t: (1 - 1 / (0.1 * torch.pi**2)) * torch.sin(torch.pi * x) * torch.exp(-torch.pi**2 * 0.1 * t\n",
    "                    ) + torch.sin(4 * torch.pi * x) * torch.exp(- 16 * torch.pi**2 * 0.1 * t\n",
    "                    ) + 1 / (0.1 * torch.pi**2) * torch.sin(torch.pi * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294067d",
   "metadata": {},
   "source": [
    "Test different numbers of layers and different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd924a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 47.422729, MSE: 0.216549\n",
      "Epoch 50, Loss: 23.440220, MSE: 0.182161\n",
      "Epoch 100, Loss: 13.175997, MSE: 0.088233\n",
      "Epoch 150, Loss: 9.694346, MSE: 0.071735\n",
      "Epoch 200, Loss: 7.817989, MSE: 0.053309\n",
      "Epoch 250, Loss: 6.203895, MSE: 0.035725\n",
      "Epoch 300, Loss: 5.280070, MSE: 0.027284\n",
      "Epoch 350, Loss: 4.721897, MSE: 0.023851\n",
      "Epoch 399, Loss: 4.097630, MSE: 0.021244\n",
      "Epoch 0, Loss: 1.884087, MSE: 0.003763\n",
      "Epoch 1, Loss: 0.823469, MSE: 0.000954\n",
      "Epoch 2, Loss: 0.421720, MSE: 0.000261\n",
      "Epoch 3, Loss: 0.242479, MSE: 0.000193\n",
      "Epoch 4, Loss: 0.148513, MSE: 0.000118\n",
      "Epoch 5, Loss: 0.095363, MSE: 0.000047\n",
      "Epoch 6, Loss: 0.064038, MSE: 0.000024\n",
      "Epoch 7, Loss: 0.046681, MSE: 0.000020\n",
      "Epoch 8, Loss: 0.036265, MSE: 0.000013\n",
      "Epoch 9, Loss: 0.025542, MSE: 0.000010\n",
      "Layers:             [32, 64, 1]\n",
      "L^2 error:          0.00139357\n",
      "L_^{infty}_error:   0.013494074\n",
      "Runtime:            154.4992936310009 \n",
      "\n",
      "Epoch 0, Loss: 47.525280, MSE: 0.201070\n",
      "Epoch 50, Loss: 21.297403, MSE: 0.154924\n",
      "Epoch 100, Loss: 15.612194, MSE: 0.129111\n",
      "Epoch 150, Loss: 12.257797, MSE: 0.114284\n",
      "Epoch 200, Loss: 10.461246, MSE: 0.108328\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m pde_nn\u001b[38;5;241m.\u001b[39mset_data(N_colloc)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# gives relatively good results (compared to other parameters, still bad though)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mpde_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_time_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# LBFGS needs approximately 100 epochs, 30 iterations for kappa = 1\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# if kappa = 0.1, better choose more iterations, less epochs\u001b[39;00m\n\u001b[1;32m     40\u001b[0m pde_nn\u001b[38;5;241m.\u001b[39mtrain_lbfgs(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, opt_time_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/Project-3-Applied-ML/Code/pinn.py:205\u001b[0m, in \u001b[0;36mheat_nn.train\u001b[0;34m(self, lr, weight_decay, epochs, opt_time_scale, print_epochs, save_losses)\u001b[0m\n\u001b[1;32m    202\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    203\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn()\n\u001b[0;32m--> 205\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_losses:\n",
      "File \u001b[0;32m/dolfinx-env/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dolfinx-env/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dolfinx-env/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### create a list of different layer combinations\n",
    "### output layer has always size 1\n",
    "\n",
    "# list of layers to test\n",
    "layers_list = [[32, 64, 1],\n",
    "               [64, 64, 1],\n",
    "               [32, 64, 64, 1],\n",
    "               [64, 64, 32, 1],\n",
    "               [64, 32, 1],\n",
    "               [128, 64, 32, 1],\n",
    "               [128, 64, 1],\n",
    "               [128, 128, 64, 64, 32, 1],\n",
    "               [128, 128, 64, 32, 1],\n",
    "               [32, 64, 128, 128, 1],\n",
    "               [128, 128, 1],\n",
    "               [128, 128, 128, 1],\n",
    "               [512, 1],\n",
    "               [256, 256, 1],\n",
    "               [128, 128, 128, 128, 1]]\n",
    "\n",
    "for layers in layers_list:\n",
    "\n",
    "    # use a random seed for comparability\n",
    "    np.random.seed(238)\n",
    "    torch.manual_seed(301)\n",
    "\n",
    "    # measure computation time\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    activations = [torch.tanh]*(len(layers)-1) + [None]\n",
    "    pde_nn = heat_nn(layers, activations, dim, u_0, kappa, rhs, reg=0)\n",
    "    pde_nn.set_analytic_solution(u_analytic)\n",
    "    N_colloc = 100\n",
    "    pde_nn.set_data(N_colloc)\n",
    "\n",
    "    # gives relatively good results (compared to other parameters, still bad though)\n",
    "    pde_nn.train(lr=1e-2, weight_decay=0.0, epochs = 400, opt_time_scale =True, print_epochs=50)\n",
    "    # LBFGS needs approximately 100 epochs, 30 iterations for kappa = 1\n",
    "    # if kappa = 0.1, better choose more iterations, less epochs\n",
    "    pde_nn.train_lbfgs(lr=1, opt_time_scale = True, epochs=10, max_iter=50)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    ### error measured in L^2 and L^{\\infty} norm\n",
    "    L_2_err = pde_nn.L_2_error()\n",
    "    L_infty_err = pde_nn.L_infty_error()\n",
    "    print(\"Layers:            \", layers)\n",
    "    print(\"L^2 error:         \", L_2_err)\n",
    "    print(\"L_^{infty}_error:  \", L_infty_err)\n",
    "    print(\"Runtime:           \", end - start, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be1aa29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolfinx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
